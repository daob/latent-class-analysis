---
title: "Loglinear LCA"
editor: visual
author: "DL Oberski"
format:
  html:
    code-fold: false
    code-summary: "Show the code"
    code-tools: true
    code-link: true
    theme: zephyr
    toc: true
execute:
  cache: true
---

# Loglinear formulation of LCA

The loglinear formulation of LCA is very powerful. It only works for categorical data, but, in addition to the "plain vanilla" kind of locally independent LCA, it allows for:

-   Built-in handling of missing data (everything is a missing data problem);
-   NMAR ("unseen data-dependent") models;
-   Multiple latent class variables, unlocking arbitrary modelling with latent variables, i.e. "categorical SEM";
-   "Linear by linear" and "linear by nominal" effects, making for more parsimonious models, unlocking:
-   Discretized, "nonparametric" versions of IRT models ("DFactor models" in Latent Gold)
-   Covariates/concomitant variables, including "measurement invariance" models;
-   Facilities for complex sampling designs, i.e. weighting, clustering, and stratification;
-   and more.

The largest drawback is that, as implemented in most software (including the excellent `cvam` package in `R`), it is extremely intensive on computer memory. This means that most models that run easily in other software will likely refuse to run on your laptop, giving an "out of memory" error.

The loglinear formulation of LCA is the basis of the powerful Latent Gold software, as well as its predecessor, the still-popular LEM program (https://jeroenvermunt.nl/). Latent Gold uses many optimizations, which mostly allow it to circumvent the "small models only" problem. Here, we will use `cvam`, whose implementation is very similar to LEM.

To demonstrate the uses of loglinear LCA, we use the `carcinoma` data from the `poLCA` package.

## Simple analysis using `poLCA`

```{r}
#| message: false
library(tidyverse)
library(poLCA)
library(poLCA.extras)

set.seed(202303)

data(carcinoma)
head(carcinoma %>% sample_n(10))
```

```{r}
f_carcinoma <- cbind(A, B, C, D, E, F, G) ~ 1
fit_polca <- poLCA(f_carcinoma, 
                   nclass = 2,
                   data = carcinoma, 
                   nrep = 10,
                   verbose = FALSE)

fit_polca
```

From the poLCA model, using the bivariate residual (BVR) functionality in the `poLCA.extras` package, we can see that there are some considerable local dependencies.

```{r}
bvr(fit_polca) |> round(1)
bootstrap_bvr_pvals(f_carcinoma, 
                    fit_polca = fit_polca, 
                    data = carcinoma, R = 200)
```

## Simple and locally dependent LCA using `cvam`

```{r}
library(cvam)

options(contrasts = rep("contr.sum", 2))

dat <- carcinoma %>% mutate_all(as.factor)
df_freq <- table(dat, useNA = "ifany") |> as.data.frame()

# Create a new variable that is completely missing (and has 2 cats)
df_freq$X <- latentFactor(NROW(df_freq), 2)

# Create formula
formula <- 
  paste(names(df_freq)[!names(df_freq) %in% c("Freq", "X")],
        collapse = " + ") %>% 
  sprintf("~ X * (%s)", .) %>%
  as.formula()

system.time(
  fit_cvam <- cvam(formula, data = df_freq,
                   freq = Freq,
                   control = list(startValJitter = 0.1)
))

summary(fit_cvam)

what <- paste0("~", LETTERS[1:7], "|X") |> sapply(as.formula)
est_indep <- cvamEstimate(c(~X, what), fit_cvam)
est_indep
```

(Note that the Hessian-based se's cannot be trusted. We can look at the estimated se's in the marginal probability tables above, or use `cvam`'s MCMC option to obtain standard errors on the loglinear parmeters.)

Now we update the model with the local dependences, held equal across classes.

```{r}
# formula_ld <- update(formula, ~ . + D:F + A:G + B:E)
formula_ld <- update(formula, ~ . + (A+B+D+E+F+G)^2)

system.time(
  fit_cvam_ld <- 
    cvam(formula_ld, data = df_freq, freq = Freq,
         control = list(startValJitter = 0.05, iterMaxEM = 1000),
         prior = cvam::cvamPrior(ridge = 0.1)
))

summary(fit_cvam_ld)

est_locdep <- cvamEstimate(c(~X, what), fit_cvam_ld)
est_locdep

anova(fit_cvam, fit_cvam_ld, method = "BIC")
```

The local dependence model fits better.

## Extensions to the basic LC model using loglinear formulation

### Including covariates

### Multiple latent variables

Example from Oberski (2016), available at <https://doi.org/10.1007/s11634-015-0211-0> (open access).

```{r}
library(readr)
library(cvam)
options(contrasts = rep("contr.sum", 2))
```

Data are from the LISS panel. People were asked 5 times if they had voted in the last parliamentary election. This pertained to two separate elections. The substantively inspired model is as shown.

```{mermaid}
flowchart TD
  X1((Voted 2006)) ---> A[2008]
  X1 ---> B[2009]
  X1 ---> C[2010]
  X2((Voted 2010)) ---> D[2011]
  X2 ---> E[2012]
  X1 --> X2
```

Read in the data. 

```{r}
#| message: false
path <- "https://daob.nl/files/lca/vote-LISS.dat.gz"
vote <- readr::read_table(path, na = ".")

rmarkdown::paged_table(vote)
```

We transform the data into frequencies (including the missing values). We also rename the variables A-E to write smaller formulas.

```{r}
df_freq <- table(vote[, -1], useNA = "ifany") |> as.data.frame()
names(df_freq)[1:5] <- LETTERS[1:5]
```

We now formulate the model. It includes two different latent variables `X1` and `X2`, which are meant to signify really voting in 2006 and 2010, respectively. 

```{r}
# Create a new variable that is completely missing (and has 2 cats)
df_freq$X1 <- latentFactor(NROW(df_freq), 2)
df_freq$X2 <- latentFactor(NROW(df_freq), 2)

# Create formula
formula <- ~ X1 * (A + B + C) + X2 * (D + E) + X1:X2
```

We use `cvam` to fit the model.
```{r}
fit_cvam <- cvam(formula, data = df_freq, freq = Freq,
                 control = list(startValJitter = 0.1))
```

Some results of interest.

```{r}
summary(fit_cvam)

what <- paste0("~", LETTERS[1:3], "|X1") |> sapply(as.formula)
what <- what %>% c(paste0("~", LETTERS[4:5], "|X2") |> sapply(as.formula))
what <- c(~X1, ~X2, ~X2 | X1, what)

est_indep <- cvamEstimate(what, fit_cvam)
est_indep
```

As in the paper, we can also include two local dependencies by updating the formula and refitting.

```{r}
# Update formula
formula_ld <- update(formula, ~ . + A:B + A:E)

fit_cvam_ld <- cvam(formula_ld, data = df_freq, freq = Freq,
                 control = list(startValJitter = 0.1))
```

The models are nested, so we can use the LRT to compare them.

```{r}
anova(fit_cvam, fit_cvam_ld)
```

The actual estimates are not too different.

```{r}
est_locdep <- cvamEstimate(what, fit_cvam_ld)
est_locdep
```


### Ordinal indicators

First consider the four-category indicators as nominal.

```{r}
data("election")

df_freq <- election[, 1:5] %>% table() %>% as.data.frame()
df_freq$X <- latentFactor(NROW(df_freq), 2)

# Create formula
formula <- ~ X * (MORALG + CARESG + KNOWG + LEADG + DISHONG)

fit_cvam <- cvam(formula, data = df_freq, freq = Freq,
                 control = list(startValJitter = 0.1))

summary(fit_cvam)

what <- c(~X, ~MORALG | X, ~CARESG | X, ~KNOWG | X, 
          ~LEADG | X, ~DISHONG | X)
cvamEstimate(what, fit_cvam)
```

Now consider them "ordinal" using an adjacent-category logit model, a.k.a. "nominal-by-linear" model in Goodman's loglinear terminology.

```{r}
scale_01 <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

factor_to_numeric_01 <- function(x) {
  x_num <- seq_along(levels(x))[x]
  scale_01(x_num)
}

X <- fit_cvam$modelMatrix
X <- X %>% as_tibble() %>% 
  # Remove nominal interaction terms
  select_if(!grepl("X.:.+", names(.))) 
# Now include linear interaction terms by hand
X$`X1:MORALG` <- X$X1 * factor_to_numeric_01(fit_cvam$mfTrue$MORALG)
X$`X1:CARESG` <- X$X1 * factor_to_numeric_01(fit_cvam$mfTrue$CARESG)
X$`X1:KNOWG` <- X$X1 * factor_to_numeric_01(fit_cvam$mfTrue$KNOWG)
X$`X1:LEADG` <- X$X1 * factor_to_numeric_01(fit_cvam$mfTrue$LEADG)
X$`X1:DISHONG` <- X$X1 * factor_to_numeric_01(fit_cvam$mfTrue$DISHONG)

rmarkdown::paged_table(X)

X <- as.matrix(X)
```

We can now fit the model with ordinal indicators by passing our custom design matrix `X` to `cvam`.

```{r}
fit_cvam_ord <- cvam(formula, data = df_freq, 
                     freq = Freq, modelMatrix = X, 
                     control = list(startValJitter = 0.1))

summary(fit_cvam_ord)

cvamEstimate(what, fit_cvam_ord)
```

The ordinal model is more parsimonious, using `r fit_cvam_ord$degreesOfFreedom - fit_cvam$degreesOfFreedom` fewer parameters. 

```{r}
anova(fit_cvam_ord, fit_cvam)
```
```{r}
anova(fit_cvam_ord, fit_cvam, method = "BIC")
```


### Linear effects among observed and latent variables (DFactor models)




### Missing-not-at-random (MNAR) / Unseen data-dependent models

## Doing everything by hand

```{r}
#| message: false

options(contrasts = rep("contr.sum", 2))

varnames <- LETTERS[1:5]
sum_vars <- paste(varnames, collapse = "+")

formula <- sprintf("Freq ~ X * (%s)", sum_vars)
# formula <- sprintf("Freq ~ X * (%s) + B:E + G:A + D:F + B:G + G:D", sum_vars)

df_freq <- carcinoma[, varnames] |> table() |> as.data.frame()
n <- nrow(df_freq)
df_freq$patnum <- 1:n

df_expanded <- rbind(df_freq, df_freq)
df_expanded$X <- rep(0:1, each = n)
post_start <- runif(n)
df_expanded$post <- c(post_start, 1-post_start)

suppressWarnings( # LCAs often have boundary values within classes
  for(i in 1:200) {
    # M-step
    # Loglinear model
    fit_glm <- glm(formula, 
                   data = df_expanded, weights = post, 
                   family = "poisson")
    
    # E-step
    eta.X <- predict(fit_glm) # Linear predictor given X
    eta.X <- matrix(eta.X, nrow = n) |> t()
    n.X <- sum(exp(eta.X)) # Sample size given X 
    P_YX <- exp(eta.X)/n.X # Probability of each pattern, joint (X, Y)
    
    P_Y <- colSums(P_YX)
    P_X.Y <- t(P_YX) / P_Y # Posterior of X given Y
    
    df_expanded$post <- as.vector(P_X.Y)
})

summary(fit_glm)

P_X <- rowSums(P_YX) # Prior
P_Y.X <- P_YX/P_X    # Conditional of Y given X

P_Y.X_marginal <- sapply(varnames, function(v) {
  apply(P_Y.X, 1, function(x) tapply(x, df_freq[, v], sum))
})

P_Y.X_marginal |> round(3)

# fit_polca$probs |> lapply(round, 3)

N <- sum(df_freq$Freq)

expected <- P_Y * N
observed <- df_freq$Freq

loglik <- sum(observed * log(P_Y))
deviance <- -2*loglik
p <- length(coef(fit_glm))
bic_deviance <- deviance + p*log(N)
bic_deviance

X2 <- sum((observed - expected)^2/expected)
G2 <- 2*sum(observed * log(observed/expected), na.rm = TRUE)

data.frame(Chisq=c(X2, fit_polca$Chisq), Gsqp=c(G2, fit_polca$Gsq), method=c("loglinear", "poLCA"))
```

### Alternative R implementation

This does appear to work as well. It requires more hand-holding than `cvam`.

```{r}
library(gllm)

options(contrasts = rep("contr.treatment", 2))

K <- 3

df_expanded <- Reduce(rbind, replicate(K, df_freq, simplify = FALSE))
df_expanded$X <- factor(rep(seq(K), each = n))

formula_gllm <- ~ X * (A + B + C + D + E)
X <- model.matrix(formula_gllm, data = df_expanded)
s <- replicate(K, 1:nrow(df_freq))

res <- emgllm(y = df_freq$Freq, s = s, X = X)
res

with(res, sum((fitted.values - observed.values)^2/fitted.values))
with(res, 2*sum(observed.values * log(observed.values/fitted.values), na.rm = TRUE))

```
